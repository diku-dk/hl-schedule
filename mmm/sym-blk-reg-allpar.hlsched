/**
 * An LMAD L = off + { (n_1 : s_1), ... (n_k, s_k) }
 *   denotes the set of 1D points (memory locations):
 *   { off + i_1*s_1 + ... + i_k*s_k |
 *     0 <= i_1 < n_1, ..., 0 <= i_k < n_k }
 */

// this is supposed to be the high-level schedule for
// kernel mmmSymBlkRegAllParKer (in kernels.cu.h)
hlsched@GPU SymBlkRegAllPar ( M : int, N : int, Q : int
                            , A : [M][Q]real@GlbM
                            , B : [Q][N]real@GlbM
                            , C : [M][N]real@GlbM
) {
    /**
     * Extension for splitable:
     *     splitable M@r1 = m*Tby, M@r2 = m1*T1, ...;
     * Explanation:
     *   if we need to split dimension M in two different ways
     *   then we need to disambiguate between them by indicating
     *   the rules `r1` or `r2`.
     */
    splitable M = m * Tby, N = n * Tbx, Q = q * Ts;

    Grid g(q, m, n) {
        W(C) = C[:,:].split(1).split(0).pushRed(q,+).reorder([1,3,2,4,0]).mapPar(0->z, 1->y, 2->x);
        R(A) = A.split(1).split(0).reorder([1,2,0,3]).mapPar(0->z, 1->y);
        R(B) = B.split(1).split(0).reorder([0,2,1,3]).mapPar(0->z, 1->x);
        /**
         * W(C) denotes the write set (of indices) of array C,
         * R(A) and R(B) denote the read set (of indices) of arrays
         *      A and B (all at grid level).
         * Observations: 
         * 1. reordering is not necessary (at this stage) as long as we map
         *    parallelism on the grid consistently, as it is implicit that
         *    the "grid" dimensions should be outermost. But reordering makes
         *    it easier later.
         * 2. The above definitions of the write and read sets produce the
         *    following LMAD-based index sets:
         *    W(C) => 0 + { (q : 0+)^{g.z}, (m : N*Tby)^{g.y}, (n : Tbx)^{g.x}, (Tby : N), (Tbx : 1) }
         *    R(A) => 0 + { (q : Ts)^{g.z}, (m : Q*Tby)^{g.y}, (Tby : Q), (Ts : 1) }
         *    R(B) => 0 + { (q : N*Ts)^{g.z}, (n : Tbx)^{g.x}, (Ts : N), (Tbx : 1) }
         */
    }

    // Blocks and threads have properties/fields `idx`, `idy`, `idz`.
    Block b (1, Ty, Tx) {
        W(C) = g.W(C)[b.idz,b.idy,b.idx].split(1).split(0).reorder(0,2,1,3).mapPar(0->y, 1->x);
            // : [Ty][Tx][Ry][Rx]real, with Ty mapped on b.y and Tx mapped on b.x
        R(A) = g.R(A).[b.idz,b.idy].split(1).split(0).reorder(1,2,3,0).mapPar(1 -> y);
            // : [Rk][Ty][Ry][Tk]real, with Ty mapped on b.y
        R(B) = g.R(B).[b.idz,b.idx].split(1).split(0).reorder(1,0,2,3).mapPar(2 -> x);
            // : [Rk][Tk][Tx][Rx]real, with Tx mapped on b.x
        /**
         * Notation:
         * If a dot (".") is used to slice a parallel dimension, then the dot
         *   is shortcut notation for the corresponding Cuda block-id or thread-id,
         *   e.g., "g.W(C)[.,.,.]" infers "g.W(C)[b.idz,b.idy,b.idx]"
         */

        // Remapping arrays A and B from global to shared memory
        //   (at block level by an implied collective copy)
        Ash@ShrM := R(A)[.];
            // shared-memory buffer of size: [Ty][Ry][Tk]real
        Bsh@ShrM := R(B)[.];
            // shared-memory buffer of size: [Tk][Tx][Rx]real
        /** 
         * If a dot (".") is used to slice a sequential dimension, then the slice
         *   is taken inside a normalized loop and the dot denotes the loop index.
         *   for(int i=0; i<Rk; i++) {
         *       Ash@ShrM = R(A)[i];
         *       Bsh@ShrM = R(B)[i];
         *   }
         */
    }

    //The per-thread result of C is remapped to register memory 
    Thread t ( result(C) : [Ry][Rx]real@RegM ) {
        // Parallel copy from shared to registers
        Areg@RegM := Ash[.,:,.]; // : [Ry]real
        Breg@RegM := Bsh[.,.,:]; // : [Rx]real
        /**
         * "Areg" and "Breg" are computed inside a sequential loop:
         * for(int i=0; i<Tk; i++) {
         *     Areg = Ash[t.idy,:,i];
         *     Breg = Bsh[i,t.idx,:];
         * }
         */
        b.W(C)[t.idy,t.idx] := result(C)
    }
}

